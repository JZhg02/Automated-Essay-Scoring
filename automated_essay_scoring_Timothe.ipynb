{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "896d4c671be8a2eb",
   "metadata": {},
   "source": [
    "# Automated Essay Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.035051700Z",
     "start_time": "2024-05-25T19:09:44.975748400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17605286808ffd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.347609100Z",
     "start_time": "2024-05-25T19:09:44.993260Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d2d03515cada84f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.374356400Z",
     "start_time": "2024-05-25T19:09:45.348609800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000d118</td>\n",
       "      <td>Many people have car where they live. The thin...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fe60</td>\n",
       "      <td>I am a scientist at NASA that is discussing th...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001ab80</td>\n",
       "      <td>People always wish they had the same technolog...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001bdc0</td>\n",
       "      <td>We all heard about Venus, the planet without a...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002ba53</td>\n",
       "      <td>Dear, State Senator\\r\\n\\r\\nThis is a letter to...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  essay_id                                          full_text  score\n",
       "0  000d118  Many people have car where they live. The thin...      3\n",
       "1  000fe60  I am a scientist at NASA that is discussing th...      3\n",
       "2  001ab80  People always wish they had the same technolog...      4\n",
       "3  001bdc0  We all heard about Venus, the planet without a...      4\n",
       "4  002ba53  Dear, State Senator\\r\\n\\r\\nThis is a letter to...      3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "215f3ae5c96b5259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.431477300Z",
     "start_time": "2024-05-25T19:09:45.363275800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17307 entries, 0 to 17306\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   essay_id   17307 non-null  object\n",
      " 1   full_text  17307 non-null  object\n",
      " 2   score      17307 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 405.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3625639a20e2f218",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.525932100Z",
     "start_time": "2024-05-25T19:09:45.381368400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17307.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.948402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.044899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score\n",
       "count  17307.000000\n",
       "mean       2.948402\n",
       "std        1.044899\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        3.000000\n",
       "75%        4.000000\n",
       "max        6.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c24d6d9d4d79a93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.526441Z",
     "start_time": "2024-05-25T19:09:45.399518900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_id     0\n",
       "full_text    0\n",
       "score        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd6933bf622553",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Objective : Automated Essay Scoring\n",
    "\n",
    "Use LLM to classify the essays between 0-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ebc1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate accelerate\n",
    "#https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
    "#huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ba51b1f5da40cc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T19:09:45.603309600Z",
     "start_time": "2024-05-25T19:09:45.417141900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcedff888fb4bc79d12f2d163a3b998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17307 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534784cfaff43b6a23172773b6b473d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 112\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set, label_cols=[\"labels\"])\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03mcallbacks = [metric_callback]\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m#model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_validation_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1229\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1228\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filej4p7dpb2.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1630\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_using_dummy_loss \u001b[38;5;129;01mand\u001b[39;00m parse(tf\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m parse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.11.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;66;03m# Newer TF train steps leave this out\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m     data \u001b[38;5;241m=\u001b[39m expand_1d(data)\n\u001b[1;32m-> 1630\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_x_y_sample_weight\u001b[49m(data)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;66;03m# If the inputs are mutable dictionaries, make a shallow copy of them because we will modify\u001b[39;00m\n\u001b[0;32m   1632\u001b[0m \u001b[38;5;66;03m# them during input/label pre-processing. This avoids surprising the user by wrecking their data.\u001b[39;00m\n\u001b[0;32m   1633\u001b[0m \u001b[38;5;66;03m# In addition, modifying mutable Python inputs makes XLA compilation impossible.\u001b[39;00m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\timot\\miniconda3\\envs\\ml2\\lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 1630, in train_step\n        x, y, sample_weight = keras.utils.unpack_x_y_sample_weight(data)\n\n    AttributeError: module 'keras.utils' has no attribute 'unpack_x_y_sample_weight'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"full_text\"], truncation=True)\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0 : \"very_bad\", 1: \"very_bad\", 2 : \"bad\", 3 : \"neutral\", 4 : \"good\", 5 : \"very_good\", 6 : \"excellent\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=7, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"idc\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "#trainer.train()\n",
    "\n",
    "from transformers import create_optimizer\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_train[\"full_text\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=7, id2label=id2label, label2id=label2id\n",
    ")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_train,\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_test,\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)  # No loss argument!\n",
    "\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "\"\"\"metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set, label_cols=[\"labels\"])\n",
    "\n",
    "callbacks = [metric_callback]\"\"\"\n",
    "\n",
    "#model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b80465c02f33bfdd8468b85332293a9622f123e0573dde72fe3e7cc82553a0b0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.19 ('ml2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
